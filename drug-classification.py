import sys
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Perceptron
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.metrics import f1_score


def read_documents(csv_file):
    try: 
        df = pd.read_csv(csv_file)
        print("<< Original Dataframe >>\n" + str(df.sample(5)) + "\n\n")
    except:
        print("File not found.")
        sys.exit(0)
    return df


def plot_distribution(df):
    # Count the frequency of different Drug classes and plot them
    df['Drug'].value_counts().plot(kind='bar')
    plt.ylabel('Classes')
    plt.xlabel("Frequency")
    plt.title('Distribution of Class Drug')
    # Save graph
    plt.savefig('drug-distribution.pdf')
    # Show graph
    plt.show()


def convert_numerical(df):
    # Get dummy classes: F, M, drugA, drugB, drugC, drugX, drugY
    df_dummies = pd.get_dummies(df, prefix='', prefix_sep='', columns=['Sex'])
    # Convert categorical BP class into numerical:  LOW:0, NORMAL:1, HIGH:2
    df_dummies.BP = pd.Categorical(df_dummies.BP, ordered=True, categories=['LOW', 'NORMAL', 'HIGH']).codes
    # Convert categorical Cholesterol class into numerical:  NORMAL:0, HIGH:1
    df_dummies.Cholesterol = pd.Categorical(df_dummies.Cholesterol, ordered=True, categories=['NORMAL', 'HIGH']).codes
    # Convert categorical Drug class into numerical:  drugA: 0, drugB: 1, drugC: 2, drugX: 3, drugY: 4
    df_dummies.Drug =  pd.Categorical(df_dummies.Drug, ordered=False, categories=['drugA', 'drugB', 'drugC', 'drugX', 'drugY']).codes
    print("<< Updated Dataframe after numerical conversion >>\n" + str(df_dummies.sample(5)) + "\n\n")
    return df_dummies


def split_dataset(df):
    x_df = df.drop('Drug', axis=1)
    y_df = df['Drug']
    print("<< Dataset X(Data) >>\n" + str(x_df) + "\n")
    print("<< Dataset Y(target) >>\n" + str(y_df))
    x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=None)  # no shuffling before dataset split
    print("\n\n===> Dataset splitted")
    print("     x_train size: " + str(x_train.shape))
    print("     y_train size: " + str(y_train.shape))
    print("     x_test size: " + str(x_test.shape))
    print("     y_test size: " + str(y_test.shape))
    return x_train, x_test, y_train, y_test


def train_nb(x_train, y_train, smoothing=1.0): # Default smoothing value: 1
    # Create classifier model list
    gaussianNB =  GaussianNB()
    baseDT = DecisionTreeClassifier()
    topDT = DecisionTreeClassifier()
    perceptron = Perceptron()
    baseMLP = MLPClassifier()
    topMLP = MLPClassifier()
    clfmodel_list = [gaussianNB, baseDT, topDT, perceptron, baseMLP, topMLP]
    # Create parameters list for certain classifier which requires them
    paramsTopDT = {'criterion': ['gini', 'entropy'], 'max_depth': [3, 6], 'min_samples_leaf': [0.02, 1, 10]}
    paramsBaseMLP = {'activation': ['logistic'], 'solver': ['sgd']}
    paramsTopMLP = {'activation': ['logistic', 'tanh', 'relu', 'identity'], 'hidden_layer_sizes': [(30, 50), (10, 10, 10)], 'solver': ['sgd', 'adam']}
    parameters_list = [{}, {}, paramsTopDT, {}, paramsBaseMLP, paramsTopMLP] # 0th, 1th, 3th classifier models uses default parameters thus {}
    # Train and store each results generated by each classifiers
    results_grid=[]
    for i in range(len(clfmodel_list)):
        grid=GridSearchCV(estimator=clfmodel_list[i], param_grid=parameters_list[i])
        grid.fit(x_train, y_train) # train it!
            #storing result
        results_grid.append\
        (
            {
                'grid': grid,
                'classifier': grid.best_estimator_,
                'best score': grid.best_score_,
                'best params': grid.best_params_,
            }
        )
    return x_train, y_train, results_grid


def test_nb(x_test, y_test, results_grid, enable_file_write=True):
    # These lists are used for step 8
    accuracy_list, f1_macro_score_list, f1_weighted_score_list = [], [], []
    # Test set for each classifiers
    for result in results_grid:
        grid = result.get('grid')
        x_test_pred = grid.predict(x_test)
        conf_matrix = confusion_matrix(y_test, x_test_pred)
        report = classification_report(y_test, x_test_pred)
        acc_score = round(accuracy_score(y_test, x_test_pred), 4)
        f1_macro_score = round(f1_score(y_test, x_test_pred, average='macro'), 4)
        f1_weighted_score = round(f1_score(y_test, x_test_pred, average='weighted'), 4)
        # 7. For each of the 6 classifier, append all the infos in the file
        if enable_file_write:
            with open('drugs-performance.txt', 'a') as file:
                file.write("---------- " + str(result.get('classifier')) + " ----------\n")
                if result == results_grid[2] or result == results_grid[5]:
                    file.write("Best Parameter chosen for this classifier: " + str(result.get("best params")) + "\n")
                    file.write("Best score obtained from the chosen parameter: " + str(result.get("best score")) + "\n")
                file.write("Result Prediction (Actual value & Predicted value):\n")
                file.write("(b) Confusion matrix: \n" + str(conf_matrix) + "\n")
                file.write("(c) Classification report (Precision, Recall, F1): \n\n" + str(report))
                file.write("(d) Accuracy score: " + str(acc_score) + " (" + str(round(acc_score*100, 2)) + "%)\n")
                file.write("    F1-score with macro average: " + str(f1_macro_score) + " (" + str(round(f1_macro_score*100, 2)) + "%)\n")
                file.write("    F1-score with weighted average: " + str(f1_weighted_score) + " (" + str(round(f1_weighted_score*100, 2)) + "%)\n\n\n")
        # Followings are code for step 8: append results to list
        accuracy_list.append(acc_score)
        f1_macro_score_list.append(f1_macro_score)
        f1_weighted_score_list.append(f1_weighted_score)
    return accuracy_list, f1_macro_score_list, f1_weighted_score_list



################ MAIN ################

# Reset file content for each execution
file = open("drugs-performance.txt","w")
file.close()

# 2. Load the dataset
dataset = read_documents('drug200.csv')
# 3. Plot the distribution of the instances in each class and store the graphic in a file
plot_distribution(dataset)
# 4. Convert  all  ordinal  and  nominal  features  in  numerical  format
dataset = convert_numerical(dataset)
# 5. Split the dataset
x_train, x_test, y_train, y_test = split_dataset(dataset)
# 6. Run 6 different classifiers
x_train, y_train, results_grid = train_nb(x_train, y_train)
# 7. get results of each 6 classifier and append results into file
test_nb(x_test, y_test, results_grid)

# 8.  Redo training steps 10 times for each model 
ten_accuracy_list, ten_f1_macro_score_list, ten_f1_weighted_score_list = [], [], []
average_accuracy_list, average_f1_macro_score_list, average_f1_weighted_score_list = [], [], []
for i in range(10):
    x_train, y_train, results_grid = train_nb(x_train, y_train)
    accuracy_list, f1_macro_score_list, f1_weighted_score_list = test_nb(x_test, y_test, results_grid, enable_file_write=False)
    ten_accuracy_list.append(accuracy_list)
    ten_f1_macro_score_list.append(f1_macro_score_list)
    ten_f1_weighted_score_list.append(f1_weighted_score_list)
new_accuracy_list = zip(*ten_accuracy_list)
new_f1_macro_score_list = zip(*ten_f1_macro_score_list)
new_f1_weighted_score_list = zip(*ten_f1_weighted_score_list)
for accuracy in new_accuracy_list:
    average_accuracy = round(sum(accuracy) / len(accuracy), 4)
    average_accuracy_list.append(average_accuracy)
for f1_macro_score in new_f1_macro_score_list:
    average_f1_macro_score = round(sum(f1_macro_score) / len(f1_macro_score), 4)
    average_f1_macro_score_list.append(average_f1_macro_score)
for f1_weighted_score in new_f1_weighted_score_list:
    average_f1_weighted_score = round(sum(f1_weighted_score) / len(f1_weighted_score), 4)
    average_f1_weighted_score_list.append(average_f1_weighted_score)
average_accuracy = round(sum(average_accuracy_list) / len(average_accuracy_list), 4)
average_f1_macro_score = round(sum(average_f1_macro_score_list) / len(average_f1_macro_score_list), 4)
average_f1_weighted_score = round(sum(average_f1_weighted_score_list) / len(average_f1_weighted_score_list), 4)
std_accuracy = round(np.std(average_accuracy_list), 4)
std_f1_macro_score = round(np.std(average_f1_macro_score_list), 4)
std_f1_weighted_score = round(np.std(average_f1_weighted_score_list), 4)
# Output in file
with open('drugs-performance.txt', 'a') as file:
    file.write("\n\n================== Question (8) ==================\n\n")
    # Accuracy
    file.write("List of accuracy for each classifier types after 10 iterations: " + str(ten_accuracy_list)+ "\n")
    file.write("Average accuracy for each classifier types after 10 iterations: " + str(average_accuracy_list) + "\n")
    file.write("Average total accuracy: " + str(average_accuracy) + "\n")
    file.write("Standard deviation of accuracy: " + str(std_accuracy) + "\n\n")
    # F1 macro score
    file.write("List of f1 macro score for each classifier types after 10 iterations: " + str(ten_f1_macro_score_list)+ "\n")
    file.write("Average f1 macro score for each classifier types after 10 iterations: " + str(average_f1_macro_score_list) + "\n")
    file.write("Average total f1 macro score: " + str(average_f1_macro_score) + "\n")
    file.write("Standard deviation of f1 macro score: " + str(std_f1_macro_score) + "\n\n")
    # F1 weighted score
    file.write("List of f1 weighted score for each classifier types after 10 iterations: " + str(ten_f1_weighted_score_list)+ "\n")
    file.write("Average f1 weighted score for each classifier types after 10 iterations: " + str(average_f1_weighted_score_list) + "\n")
    file.write("Average total f1 weighted score: " + str(average_f1_weighted_score) + "\n")
    file.write("Standard deviation of f1 weighted score: " + str(std_f1_weighted_score) + "\n\n")