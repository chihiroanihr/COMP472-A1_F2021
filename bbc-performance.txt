---------- Multi-nomial NB default values, try 1 ----------

Result Prediction (Actual value & Predicted value)
(b) Confusion matrix: 
[[102   1   2   0   2]
 [  0  73   0   0   3]
 [  1   1  77   0   0]
 [  1   0   0 100   0]
 [  1   0   1   0  80]]
(c) Classification report (Precision, Recall, F1): 
              precision    recall  f1-score   support

           0       0.97      0.95      0.96       107
           1       0.97      0.96      0.97        76
           2       0.96      0.97      0.97        79
           3       1.00      0.99      1.00       101
           4       0.94      0.98      0.96        82

    accuracy                           0.97       445
   macro avg       0.97      0.97      0.97       445
weighted avg       0.97      0.97      0.97       445

(d) Accuracy score: 0.9708 (97.08%)
    F1-score with macro average: 0.9702 (97.02%)
    F1-score with weighted average: 0.9708 (97.08%)
(e) The Prior Probability of each class IN TRAIN SET
    for business (0): 0.1742
    for entertainment (1): 0.1742
    for politics (2): 0.1899
    for sport (3): 0.2303
    for tech (4): 0.1792
(f) The size of the vocabulary IN TRAIN SET (i.e. the number of different words): 29421
(g) The number of word-tokens in each class IN TRAIN SET (i.e.  the number of words in total)
    for business (0): 131652
    for entertainment (1): 101978
    for politics (2): 153132
    for sport (3): 130771
    for tech (4): 160734
(h) The number of word-tokens in the entire corpus (TRAIN SET + TEST SET): 836357
(i) The number and percentage of 'words with a frequency of zero' in each class IN TRAIN SET
    for business (0): number = 18707, percentage = 14.21%
    for entertainment (1): number = 18878, percentage = 18.51%
    for politics (2): number = 19011, percentage = 12.41%
    for sport (3): number = 19782, percentage = 15.13%
    for tech (4): number = 18152, percentage = 11.29%
(j) The number and percentage of 'words with a frequency of one' in the entire corpus (TRAIN SET + TEST SET):
    number = 319824, percentage = 38.24%
(k) Your 2 favorite words (that are present in the vocabulary) and their log-prob IN TRAIN SET
 1. Log probability of word1 ("japan") with smoothing value = 1: 
    log(P("japan"|business)) = 0.000558752863608426
    log(P("japan"|entertainment)) = 0.00017503938386136881
    log(P("japan"|politics)) = 1.0955722447727509e-05
    log(P("japan"|sport)) = 9.363763483819416e-05
    log(P("japan"|tech)) = 0.00027346112382004153
 2. Log probability of word2 ("korea") with smoothing value = 1: 
    log(P("korea"|business)) = 0.00019245931968734674
    log(P("korea"|entertainment)) = 3.805203996986278e-05
    log(P("korea"|politics)) = 1.0955722447727509e-05
    log(P("korea"|sport)) = 2.4970035956851777e-05
    log(P("korea"|tech)) = 5.784754542347033e-05



---------- Multi-nomial NB default values, try 2 ----------

Result Prediction (Actual value & Predicted value)
(b) Confusion matrix: 
[[102   1   2   0   2]
 [  0  73   0   0   3]
 [  1   1  77   0   0]
 [  1   0   0 100   0]
 [  1   0   1   0  80]]
(c) Classification report (Precision, Recall, F1): 
              precision    recall  f1-score   support

           0       0.97      0.95      0.96       107
           1       0.97      0.96      0.97        76
           2       0.96      0.97      0.97        79
           3       1.00      0.99      1.00       101
           4       0.94      0.98      0.96        82

    accuracy                           0.97       445
   macro avg       0.97      0.97      0.97       445
weighted avg       0.97      0.97      0.97       445

(d) Accuracy score: 0.9708 (97.08%)
    F1-score with macro average: 0.9702 (97.02%)
    F1-score with weighted average: 0.9708 (97.08%)
(e) The Prior Probability of each class IN TRAIN SET
    for business (0): 0.1742
    for entertainment (1): 0.1742
    for politics (2): 0.1899
    for sport (3): 0.2303
    for tech (4): 0.1792
(f) The size of the vocabulary IN TRAIN SET (i.e. the number of different words): 29421
(g) The number of word-tokens in each class IN TRAIN SET (i.e.  the number of words in total)
    for business (0): 131652
    for entertainment (1): 101978
    for politics (2): 153132
    for sport (3): 130771
    for tech (4): 160734
(h) The number of word-tokens in the entire corpus (TRAIN SET + TEST SET): 836357
(i) The number and percentage of 'words with a frequency of zero' in each class IN TRAIN SET
    for business (0): number = 18707, percentage = 14.21%
    for entertainment (1): number = 18878, percentage = 18.51%
    for politics (2): number = 19011, percentage = 12.41%
    for sport (3): number = 19782, percentage = 15.13%
    for tech (4): number = 18152, percentage = 11.29%
(j) The number and percentage of 'words with a frequency of one' in the entire corpus (TRAIN SET + TEST SET):
    number = 319824, percentage = 38.24%
(k) Your 2 favorite words (that are present in the vocabulary) and their log-prob IN TRAIN SET
 1. Log probability of word1 ("japan") with smoothing value = 1: 
    log(P("japan"|business)) = 0.000558752863608426
    log(P("japan"|entertainment)) = 0.00017503938386136881
    log(P("japan"|politics)) = 1.0955722447727509e-05
    log(P("japan"|sport)) = 9.363763483819416e-05
    log(P("japan"|tech)) = 0.00027346112382004153
 2. Log probability of word2 ("korea") with smoothing value = 1: 
    log(P("korea"|business)) = 0.00019245931968734674
    log(P("korea"|entertainment)) = 3.805203996986278e-05
    log(P("korea"|politics)) = 1.0955722447727509e-05
    log(P("korea"|sport)) = 2.4970035956851777e-05
    log(P("korea"|tech)) = 5.784754542347033e-05



---------- Multi-nomial NB smoothing value = 0.0001 ----------

Result Prediction (Actual value & Predicted value)
(b) Confusion matrix: 
[[101   1   3   0   2]
 [  0  73   0   0   3]
 [  1   0  77   0   1]
 [  1   0   0 100   0]
 [  0   0   1   0  81]]
(c) Classification report (Precision, Recall, F1): 
              precision    recall  f1-score   support

           0       0.98      0.94      0.96       107
           1       0.99      0.96      0.97        76
           2       0.95      0.97      0.96        79
           3       1.00      0.99      1.00       101
           4       0.93      0.99      0.96        82

    accuracy                           0.97       445
   macro avg       0.97      0.97      0.97       445
weighted avg       0.97      0.97      0.97       445

(d) Accuracy score: 0.9708 (97.08%)
    F1-score with macro average: 0.9703 (97.03%)
    F1-score with weighted average: 0.9709 (97.09%)
(e) The Prior Probability of each class IN TRAIN SET
    for business (0): 0.1742
    for entertainment (1): 0.1742
    for politics (2): 0.1899
    for sport (3): 0.2303
    for tech (4): 0.1792
(f) The size of the vocabulary IN TRAIN SET (i.e. the number of different words): 29421
(g) The number of word-tokens in each class IN TRAIN SET (i.e.  the number of words in total)
    for business (0): 131652
    for entertainment (1): 101978
    for politics (2): 153132
    for sport (3): 130771
    for tech (4): 160734
(h) The number of word-tokens in the entire corpus (TRAIN SET + TEST SET): 836357
(i) The number and percentage of 'words with a frequency of zero' in each class IN TRAIN SET
    for business (0): number = 18707, percentage = 14.21%
    for entertainment (1): number = 18878, percentage = 18.51%
    for politics (2): number = 19011, percentage = 12.41%
    for sport (3): number = 19782, percentage = 15.13%
    for tech (4): number = 18152, percentage = 11.29%
(j) The number and percentage of 'words with a frequency of one' in the entire corpus (TRAIN SET + TEST SET):
    number = 319824, percentage = 38.24%
(k) Your 2 favorite words (that are present in the vocabulary) and their log-prob IN TRAIN SET
 1. Log probability of word1 ("japan") with smoothing value = 1: 
    log(P("japan"|business)) = 0.000558752863608426
    log(P("japan"|entertainment)) = 0.00017503938386136881
    log(P("japan"|politics)) = 1.0955722447727509e-05
    log(P("japan"|sport)) = 9.363763483819416e-05
    log(P("japan"|tech)) = 0.00027346112382004153
 2. Log probability of word2 ("korea") with smoothing value = 1: 
    log(P("korea"|business)) = 0.00019245931968734674
    log(P("korea"|entertainment)) = 3.805203996986278e-05
    log(P("korea"|politics)) = 1.0955722447727509e-05
    log(P("korea"|sport)) = 2.4970035956851777e-05
    log(P("korea"|tech)) = 5.784754542347033e-05



---------- Multi-nomial NB smoothing value = 0.9 ----------

Result Prediction (Actual value & Predicted value)
(b) Confusion matrix: 
[[102   1   2   0   2]
 [  0  73   0   0   3]
 [  1   1  77   0   0]
 [  1   0   0 100   0]
 [  1   0   1   0  80]]
(c) Classification report (Precision, Recall, F1): 
              precision    recall  f1-score   support

           0       0.97      0.95      0.96       107
           1       0.97      0.96      0.97        76
           2       0.96      0.97      0.97        79
           3       1.00      0.99      1.00       101
           4       0.94      0.98      0.96        82

    accuracy                           0.97       445
   macro avg       0.97      0.97      0.97       445
weighted avg       0.97      0.97      0.97       445

(d) Accuracy score: 0.9708 (97.08%)
    F1-score with macro average: 0.9702 (97.02%)
    F1-score with weighted average: 0.9708 (97.08%)
(e) The Prior Probability of each class IN TRAIN SET
    for business (0): 0.1742
    for entertainment (1): 0.1742
    for politics (2): 0.1899
    for sport (3): 0.2303
    for tech (4): 0.1792
(f) The size of the vocabulary IN TRAIN SET (i.e. the number of different words): 29421
(g) The number of word-tokens in each class IN TRAIN SET (i.e.  the number of words in total)
    for business (0): 131652
    for entertainment (1): 101978
    for politics (2): 153132
    for sport (3): 130771
    for tech (4): 160734
(h) The number of word-tokens in the entire corpus (TRAIN SET + TEST SET): 836357
(i) The number and percentage of 'words with a frequency of zero' in each class IN TRAIN SET
    for business (0): number = 18707, percentage = 14.21%
    for entertainment (1): number = 18878, percentage = 18.51%
    for politics (2): number = 19011, percentage = 12.41%
    for sport (3): number = 19782, percentage = 15.13%
    for tech (4): number = 18152, percentage = 11.29%
(j) The number and percentage of 'words with a frequency of one' in the entire corpus (TRAIN SET + TEST SET):
    number = 319824, percentage = 38.24%
(k) Your 2 favorite words (that are present in the vocabulary) and their log-prob IN TRAIN SET
 1. Log probability of word1 ("japan") with smoothing value = 1: 
    log(P("japan"|business)) = 0.000558752863608426
    log(P("japan"|entertainment)) = 0.00017503938386136881
    log(P("japan"|politics)) = 1.0955722447727509e-05
    log(P("japan"|sport)) = 9.363763483819416e-05
    log(P("japan"|tech)) = 0.00027346112382004153
 2. Log probability of word2 ("korea") with smoothing value = 1: 
    log(P("korea"|business)) = 0.00019245931968734674
    log(P("korea"|entertainment)) = 3.805203996986278e-05
    log(P("korea"|politics)) = 1.0955722447727509e-05
    log(P("korea"|sport)) = 2.4970035956851777e-05
    log(P("korea"|tech)) = 5.784754542347033e-05



