---------- Multi-nomial NB default values, try 1 ----------

Result Prediction (Actual value & Predicted value)
(b) Confusion matrix: 
[[113   2   1   0   2]
 [  0  70   0   0   2]
 [  0   0  83   0   0]
 [  0   0   0  93   0]
 [  0   1   0   0  78]]
(c) Classification report (Precision, Recall, F1): 
              precision    recall  f1-score   support

           0       1.00      0.96      0.98       118
           1       0.96      0.97      0.97        72
           2       0.99      1.00      0.99        83
           3       1.00      1.00      1.00        93
           4       0.95      0.99      0.97        79

    accuracy                           0.98       445
   macro avg       0.98      0.98      0.98       445
weighted avg       0.98      0.98      0.98       445

(d) Accuracy score: 0.982 (98.2%)
    F1-score with macro average: 0.9814 (98.14%)
    F1-score with weighted average: 0.9821 (98.21%)
(e) The Prior Probability of each class IN TRAIN SET
    for business (0): 0.2202
    for entertainment (1): 0.1764
    for politics (2): 0.1876
    for sport (3): 0.2348
    for tech (4): 0.1809
(f) The size of the vocabulary IN TRAIN SET (i.e. the number of different words): 29421
(g) The number of word-tokens in each class IN TRAIN SET (i.e.  the number of words in total)
    for business (0): 129234
    for entertainment (1): 103971
    for politics (2): 145080
    for sport (3): 131274
    for tech (4): 159363
(h) The number of word-tokens in the entire corpus (TRAIN SET + TEST SET): 836357
(i) The number and percentage of 'words with a frequency of zero' in each class IN TRAIN SET
    for business (0): number = 18903, percentage = 14.63%
    for entertainment (1): number = 18735, percentage = 18.02%
    for politics (2): number = 19441, percentage = 13.4%
    for sport (3): number = 19737, percentage = 15.03%
    for tech (4): number = 18487, percentage = 11.6%
(j) The number and percentage of 'words with a frequency of one' in the entire corpus (TRAIN SET + TEST SET):
    number = 319824, percentage = 38.24%
(k) Your 2 favorite words (that are present in the vocabulary) and their log-prob IN TRAIN SET
 1. Log probability of word1 ("japan") with smoothing value = 1: 
    log(P("japan"|business)) = 0.0005672686016828969
    log(P("japan"|entertainment)) = 0.0001799208348326736
    log(P("japan"|politics)) = 4.011438329866305e-05
    log(P("japan"|sport)) = 6.845265876349607e-05
    log(P("japan"|tech)) = 0.00025955589456733624
 2. Log probability of word2 ("korea") with smoothing value = 1: 
    log(P("korea"|business)) = 0.00018908953389429895
    log(P("korea"|entertainment)) = 3.748350725680701e-05
    log(P("korea"|politics)) = 2.865313092761646e-05
    log(P("korea"|sport)) = 1.8668906935498928e-05
    log(P("korea"|tech)) = 7.415882701923892e-05



---------- Multi-nomial NB default values, try 2 ----------

Result Prediction (Actual value & Predicted value)
(b) Confusion matrix: 
[[113   2   1   0   2]
 [  0  70   0   0   2]
 [  0   0  83   0   0]
 [  0   0   0  93   0]
 [  0   1   0   0  78]]
(c) Classification report (Precision, Recall, F1): 
              precision    recall  f1-score   support

           0       1.00      0.96      0.98       118
           1       0.96      0.97      0.97        72
           2       0.99      1.00      0.99        83
           3       1.00      1.00      1.00        93
           4       0.95      0.99      0.97        79

    accuracy                           0.98       445
   macro avg       0.98      0.98      0.98       445
weighted avg       0.98      0.98      0.98       445

(d) Accuracy score: 0.982 (98.2%)
    F1-score with macro average: 0.9814 (98.14%)
    F1-score with weighted average: 0.9821 (98.21%)
(e) The Prior Probability of each class IN TRAIN SET
    for business (0): 0.2202
    for entertainment (1): 0.1764
    for politics (2): 0.1876
    for sport (3): 0.2348
    for tech (4): 0.1809
(f) The size of the vocabulary IN TRAIN SET (i.e. the number of different words): 29421
(g) The number of word-tokens in each class IN TRAIN SET (i.e.  the number of words in total)
    for business (0): 129234
    for entertainment (1): 103971
    for politics (2): 145080
    for sport (3): 131274
    for tech (4): 159363
(h) The number of word-tokens in the entire corpus (TRAIN SET + TEST SET): 836357
(i) The number and percentage of 'words with a frequency of zero' in each class IN TRAIN SET
    for business (0): number = 18903, percentage = 14.63%
    for entertainment (1): number = 18735, percentage = 18.02%
    for politics (2): number = 19441, percentage = 13.4%
    for sport (3): number = 19737, percentage = 15.03%
    for tech (4): number = 18487, percentage = 11.6%
(j) The number and percentage of 'words with a frequency of one' in the entire corpus (TRAIN SET + TEST SET):
    number = 319824, percentage = 38.24%
(k) Your 2 favorite words (that are present in the vocabulary) and their log-prob IN TRAIN SET
 1. Log probability of word1 ("japan") with smoothing value = 1: 
    log(P("japan"|business)) = 0.0005672686016828969
    log(P("japan"|entertainment)) = 0.0001799208348326736
    log(P("japan"|politics)) = 4.011438329866305e-05
    log(P("japan"|sport)) = 6.845265876349607e-05
    log(P("japan"|tech)) = 0.00025955589456733624
 2. Log probability of word2 ("korea") with smoothing value = 1: 
    log(P("korea"|business)) = 0.00018908953389429895
    log(P("korea"|entertainment)) = 3.748350725680701e-05
    log(P("korea"|politics)) = 2.865313092761646e-05
    log(P("korea"|sport)) = 1.8668906935498928e-05
    log(P("korea"|tech)) = 7.415882701923892e-05



---------- Multi-nomial NB smoothing value = 0.0001 ----------

Result Prediction (Actual value & Predicted value)
(b) Confusion matrix: 
[[113   1   1   0   3]
 [  0  70   0   0   2]
 [  0   1  82   0   0]
 [  0   0   0  93   0]
 [  0   1   1   0  77]]
(c) Classification report (Precision, Recall, F1): 
              precision    recall  f1-score   support

           0       1.00      0.96      0.98       118
           1       0.96      0.97      0.97        72
           2       0.98      0.99      0.98        83
           3       1.00      1.00      1.00        93
           4       0.94      0.97      0.96        79

    accuracy                           0.98       445
   macro avg       0.97      0.98      0.98       445
weighted avg       0.98      0.98      0.98       445

(d) Accuracy score: 0.9775 (97.75%)
    F1-score with macro average: 0.9765 (97.65%)
    F1-score with weighted average: 0.9776 (97.76%)
(e) The Prior Probability of each class IN TRAIN SET
    for business (0): 0.2202
    for entertainment (1): 0.1764
    for politics (2): 0.1876
    for sport (3): 0.2348
    for tech (4): 0.1809
(f) The size of the vocabulary IN TRAIN SET (i.e. the number of different words): 29421
(g) The number of word-tokens in each class IN TRAIN SET (i.e.  the number of words in total)
    for business (0): 129234
    for entertainment (1): 103971
    for politics (2): 145080
    for sport (3): 131274
    for tech (4): 159363
(h) The number of word-tokens in the entire corpus (TRAIN SET + TEST SET): 836357
(i) The number and percentage of 'words with a frequency of zero' in each class IN TRAIN SET
    for business (0): number = 18903, percentage = 14.63%
    for entertainment (1): number = 18735, percentage = 18.02%
    for politics (2): number = 19441, percentage = 13.4%
    for sport (3): number = 19737, percentage = 15.03%
    for tech (4): number = 18487, percentage = 11.6%
(j) The number and percentage of 'words with a frequency of one' in the entire corpus (TRAIN SET + TEST SET):
    number = 319824, percentage = 38.24%
(k) Your 2 favorite words (that are present in the vocabulary) and their log-prob IN TRAIN SET
 1. Log probability of word1 ("japan") with smoothing value = 1: 
    log(P("japan"|business)) = 0.0005672686016828969
    log(P("japan"|entertainment)) = 0.0001799208348326736
    log(P("japan"|politics)) = 4.011438329866305e-05
    log(P("japan"|sport)) = 6.845265876349607e-05
    log(P("japan"|tech)) = 0.00025955589456733624
 2. Log probability of word2 ("korea") with smoothing value = 1: 
    log(P("korea"|business)) = 0.00018908953389429895
    log(P("korea"|entertainment)) = 3.748350725680701e-05
    log(P("korea"|politics)) = 2.865313092761646e-05
    log(P("korea"|sport)) = 1.8668906935498928e-05
    log(P("korea"|tech)) = 7.415882701923892e-05



---------- Multi-nomial NB smoothing value = 0.9 ----------

Result Prediction (Actual value & Predicted value)
(b) Confusion matrix: 
[[113   2   1   0   2]
 [  0  70   0   0   2]
 [  0   0  83   0   0]
 [  0   0   0  93   0]
 [  0   1   0   0  78]]
(c) Classification report (Precision, Recall, F1): 
              precision    recall  f1-score   support

           0       1.00      0.96      0.98       118
           1       0.96      0.97      0.97        72
           2       0.99      1.00      0.99        83
           3       1.00      1.00      1.00        93
           4       0.95      0.99      0.97        79

    accuracy                           0.98       445
   macro avg       0.98      0.98      0.98       445
weighted avg       0.98      0.98      0.98       445

(d) Accuracy score: 0.982 (98.2%)
    F1-score with macro average: 0.9814 (98.14%)
    F1-score with weighted average: 0.9821 (98.21%)
(e) The Prior Probability of each class IN TRAIN SET
    for business (0): 0.2202
    for entertainment (1): 0.1764
    for politics (2): 0.1876
    for sport (3): 0.2348
    for tech (4): 0.1809
(f) The size of the vocabulary IN TRAIN SET (i.e. the number of different words): 29421
(g) The number of word-tokens in each class IN TRAIN SET (i.e.  the number of words in total)
    for business (0): 129234
    for entertainment (1): 103971
    for politics (2): 145080
    for sport (3): 131274
    for tech (4): 159363
(h) The number of word-tokens in the entire corpus (TRAIN SET + TEST SET): 836357
(i) The number and percentage of 'words with a frequency of zero' in each class IN TRAIN SET
    for business (0): number = 18903, percentage = 14.63%
    for entertainment (1): number = 18735, percentage = 18.02%
    for politics (2): number = 19441, percentage = 13.4%
    for sport (3): number = 19737, percentage = 15.03%
    for tech (4): number = 18487, percentage = 11.6%
(j) The number and percentage of 'words with a frequency of one' in the entire corpus (TRAIN SET + TEST SET):
    number = 319824, percentage = 38.24%
(k) Your 2 favorite words (that are present in the vocabulary) and their log-prob IN TRAIN SET
 1. Log probability of word1 ("japan") with smoothing value = 1: 
    log(P("japan"|business)) = 0.0005672686016828969
    log(P("japan"|entertainment)) = 0.0001799208348326736
    log(P("japan"|politics)) = 4.011438329866305e-05
    log(P("japan"|sport)) = 6.845265876349607e-05
    log(P("japan"|tech)) = 0.00025955589456733624
 2. Log probability of word2 ("korea") with smoothing value = 1: 
    log(P("korea"|business)) = 0.00018908953389429895
    log(P("korea"|entertainment)) = 3.748350725680701e-05
    log(P("korea"|politics)) = 2.865313092761646e-05
    log(P("korea"|sport)) = 1.8668906935498928e-05
    log(P("korea"|tech)) = 7.415882701923892e-05



