---------- Multi-nomial NB default values, try 1 ----------

Result Prediction (Actual value & Predicted value)
(b) Confusion matrix: 
[[ 97   0   3   0   1]
 [  1  83   1   0   1]
 [  1   0  83   0   0]
 [  0   0   0 103   0]
 [  0   0   0   0  71]]
(c) Classification report (Precision, Recall, F1): 
              precision    recall  f1-score   support

           0       0.98      0.96      0.97       101
           1       1.00      0.97      0.98        86
           2       0.95      0.99      0.97        84
           3       1.00      1.00      1.00       103
           4       0.97      1.00      0.99        71

    accuracy                           0.98       445
   macro avg       0.98      0.98      0.98       445
weighted avg       0.98      0.98      0.98       445

(d) Accuracy score: 0.982 (98.2%)

    F1-score with macro average: 0.9818 (98.18%)
    F1-score with weighted average: 0.982 (98.2%)
(e) The Prior Probability of each class IN TRAIN SET
    for business (0): 0.1685
    for entertainment (1): 0.1685
    for politics (2): 0.1871
    for sport (3): 0.2292
    for tech (4): 0.1854
(f) The size of the vocabulary IN TRAIN SET (i.e. the number of different words): 29421
(g) The number of word-tokens in each class IN TRAIN SET (i.e.  the number of words in total)
    for business (0): 132671
    for entertainment (1): 98462
    for politics (2): 148450
    for sport (3): 129078
    for tech (4): 161379
(h) The number of word-tokens in the entire corpus (TRAIN SET + TEST SET): 836357
(i) The number and percentage of 'words with a frequency of zero' in each class IN TRAIN SET
    for business (0): number = 18708, percentage = 14.1%
    for entertainment (1): number = 19122, percentage = 19.42%
    for politics (2): number = 19131, percentage = 12.89%
    for sport (3): number = 19816, percentage = 15.35%
    for tech (4): number = 18142, percentage = 11.24%
(j) The number and percentage of 'words with a frequency of one' in the entire corpus (TRAIN SET + TEST SET):
    number = 319824, percentage = 38.24%
(k) Your 2 favorite words (that are present in the vocabulary) and their log-prob IN TRAIN SET
 1. Log probability of word1 ("japan") with smoothing value = 1: 
    log(P("japan"|business)) = 0.0005490708980085383
    log(P("japan"|entertainment)) = 0.00020331083881360305
    log(P("japan"|politics)) = 2.8110259682578948e-05
    log(P("japan"|sport)) = 8.201944491763355e-05
    log(P("japan"|tech)) = 0.0002620545073375262
 2. Log probability of word2 ("korea") with smoothing value = 1: 
    log(P("korea"|business)) = 0.00021592675764380722
    log(P("korea"|entertainment)) = 3.9098238233385206e-05
    log(P("korea"|politics)) = 1.686615580954737e-05
    log(P("korea"|sport)) = 2.5236752282348784e-05
    log(P("korea"|tech)) = 7.861635220125786e-05



---------- Multi-nomial NB default values, try 2 ----------

Result Prediction (Actual value & Predicted value)
(b) Confusion matrix: 
[[ 97   0   3   0   1]
 [  1  83   1   0   1]
 [  1   0  83   0   0]
 [  0   0   0 103   0]
 [  0   0   0   0  71]]
(c) Classification report (Precision, Recall, F1): 
              precision    recall  f1-score   support

           0       0.98      0.96      0.97       101
           1       1.00      0.97      0.98        86
           2       0.95      0.99      0.97        84
           3       1.00      1.00      1.00       103
           4       0.97      1.00      0.99        71

    accuracy                           0.98       445
   macro avg       0.98      0.98      0.98       445
weighted avg       0.98      0.98      0.98       445

(d) Accuracy score: 0.982 (98.2%)

    F1-score with macro average: 0.9818 (98.18%)
    F1-score with weighted average: 0.982 (98.2%)
(e) The Prior Probability of each class IN TRAIN SET
    for business (0): 0.1685
    for entertainment (1): 0.1685
    for politics (2): 0.1871
    for sport (3): 0.2292
    for tech (4): 0.1854
(f) The size of the vocabulary IN TRAIN SET (i.e. the number of different words): 29421
(g) The number of word-tokens in each class IN TRAIN SET (i.e.  the number of words in total)
    for business (0): 132671
    for entertainment (1): 98462
    for politics (2): 148450
    for sport (3): 129078
    for tech (4): 161379
(h) The number of word-tokens in the entire corpus (TRAIN SET + TEST SET): 836357
(i) The number and percentage of 'words with a frequency of zero' in each class IN TRAIN SET
    for business (0): number = 18708, percentage = 14.1%
    for entertainment (1): number = 19122, percentage = 19.42%
    for politics (2): number = 19131, percentage = 12.89%
    for sport (3): number = 19816, percentage = 15.35%
    for tech (4): number = 18142, percentage = 11.24%
(j) The number and percentage of 'words with a frequency of one' in the entire corpus (TRAIN SET + TEST SET):
    number = 319824, percentage = 38.24%
(k) Your 2 favorite words (that are present in the vocabulary) and their log-prob IN TRAIN SET
 1. Log probability of word1 ("japan") with smoothing value = 1: 
    log(P("japan"|business)) = 0.0005490708980085383
    log(P("japan"|entertainment)) = 0.00020331083881360305
    log(P("japan"|politics)) = 2.8110259682578948e-05
    log(P("japan"|sport)) = 8.201944491763355e-05
    log(P("japan"|tech)) = 0.0002620545073375262
 2. Log probability of word2 ("korea") with smoothing value = 1: 
    log(P("korea"|business)) = 0.00021592675764380722
    log(P("korea"|entertainment)) = 3.9098238233385206e-05
    log(P("korea"|politics)) = 1.686615580954737e-05
    log(P("korea"|sport)) = 2.5236752282348784e-05
    log(P("korea"|tech)) = 7.861635220125786e-05



---------- Multi-nomial NB smoothing value = 0.0001 ----------

Result Prediction (Actual value & Predicted value)
(b) Confusion matrix: 
[[ 94   1   3   0   3]
 [  1  83   1   0   1]
 [  1   0  83   0   0]
 [  0   0   0 103   0]
 [  1   0   0   0  70]]
(c) Classification report (Precision, Recall, F1): 
              precision    recall  f1-score   support

           0       0.97      0.93      0.95       101
           1       0.99      0.97      0.98        86
           2       0.95      0.99      0.97        84
           3       1.00      1.00      1.00       103
           4       0.95      0.99      0.97        71

    accuracy                           0.97       445
   macro avg       0.97      0.97      0.97       445
weighted avg       0.97      0.97      0.97       445

(d) Accuracy score: 0.973 (97.3%)

    F1-score with macro average: 0.9724 (97.24%)
    F1-score with weighted average: 0.973 (97.3%)
(e) The Prior Probability of each class IN TRAIN SET
    for business (0): 0.1685
    for entertainment (1): 0.1685
    for politics (2): 0.1871
    for sport (3): 0.2292
    for tech (4): 0.1854
(f) The size of the vocabulary IN TRAIN SET (i.e. the number of different words): 29421
(g) The number of word-tokens in each class IN TRAIN SET (i.e.  the number of words in total)
    for business (0): 132671
    for entertainment (1): 98462
    for politics (2): 148450
    for sport (3): 129078
    for tech (4): 161379
(h) The number of word-tokens in the entire corpus (TRAIN SET + TEST SET): 836357
(i) The number and percentage of 'words with a frequency of zero' in each class IN TRAIN SET
    for business (0): number = 18708, percentage = 14.1%
    for entertainment (1): number = 19122, percentage = 19.42%
    for politics (2): number = 19131, percentage = 12.89%
    for sport (3): number = 19816, percentage = 15.35%
    for tech (4): number = 18142, percentage = 11.24%
(j) The number and percentage of 'words with a frequency of one' in the entire corpus (TRAIN SET + TEST SET):
    number = 319824, percentage = 38.24%
(k) Your 2 favorite words (that are present in the vocabulary) and their log-prob IN TRAIN SET
 1. Log probability of word1 ("japan") with smoothing value = 1: 
    log(P("japan"|business)) = 0.0005490708980085383
    log(P("japan"|entertainment)) = 0.00020331083881360305
    log(P("japan"|politics)) = 2.8110259682578948e-05
    log(P("japan"|sport)) = 8.201944491763355e-05
    log(P("japan"|tech)) = 0.0002620545073375262
 2. Log probability of word2 ("korea") with smoothing value = 1: 
    log(P("korea"|business)) = 0.00021592675764380722
    log(P("korea"|entertainment)) = 3.9098238233385206e-05
    log(P("korea"|politics)) = 1.686615580954737e-05
    log(P("korea"|sport)) = 2.5236752282348784e-05
    log(P("korea"|tech)) = 7.861635220125786e-05



---------- Multi-nomial NB smoothing value = 0.9 ----------

Result Prediction (Actual value & Predicted value)
(b) Confusion matrix: 
[[ 97   0   3   0   1]
 [  1  83   1   0   1]
 [  1   0  83   0   0]
 [  0   0   0 103   0]
 [  0   0   0   0  71]]
(c) Classification report (Precision, Recall, F1): 
              precision    recall  f1-score   support

           0       0.98      0.96      0.97       101
           1       1.00      0.97      0.98        86
           2       0.95      0.99      0.97        84
           3       1.00      1.00      1.00       103
           4       0.97      1.00      0.99        71

    accuracy                           0.98       445
   macro avg       0.98      0.98      0.98       445
weighted avg       0.98      0.98      0.98       445

(d) Accuracy score: 0.982 (98.2%)

    F1-score with macro average: 0.9818 (98.18%)
    F1-score with weighted average: 0.982 (98.2%)
(e) The Prior Probability of each class IN TRAIN SET
    for business (0): 0.1685
    for entertainment (1): 0.1685
    for politics (2): 0.1871
    for sport (3): 0.2292
    for tech (4): 0.1854
(f) The size of the vocabulary IN TRAIN SET (i.e. the number of different words): 29421
(g) The number of word-tokens in each class IN TRAIN SET (i.e.  the number of words in total)
    for business (0): 132671
    for entertainment (1): 98462
    for politics (2): 148450
    for sport (3): 129078
    for tech (4): 161379
(h) The number of word-tokens in the entire corpus (TRAIN SET + TEST SET): 836357
(i) The number and percentage of 'words with a frequency of zero' in each class IN TRAIN SET
    for business (0): number = 18708, percentage = 14.1%
    for entertainment (1): number = 19122, percentage = 19.42%
    for politics (2): number = 19131, percentage = 12.89%
    for sport (3): number = 19816, percentage = 15.35%
    for tech (4): number = 18142, percentage = 11.24%
(j) The number and percentage of 'words with a frequency of one' in the entire corpus (TRAIN SET + TEST SET):
    number = 319824, percentage = 38.24%
(k) Your 2 favorite words (that are present in the vocabulary) and their log-prob IN TRAIN SET
 1. Log probability of word1 ("japan") with smoothing value = 1: 
    log(P("japan"|business)) = 0.0005490708980085383
    log(P("japan"|entertainment)) = 0.00020331083881360305
    log(P("japan"|politics)) = 2.8110259682578948e-05
    log(P("japan"|sport)) = 8.201944491763355e-05
    log(P("japan"|tech)) = 0.0002620545073375262
 2. Log probability of word2 ("korea") with smoothing value = 1: 
    log(P("korea"|business)) = 0.00021592675764380722
    log(P("korea"|entertainment)) = 3.9098238233385206e-05
    log(P("korea"|politics)) = 1.686615580954737e-05
    log(P("korea"|sport)) = 2.5236752282348784e-05
    log(P("korea"|tech)) = 7.861635220125786e-05



